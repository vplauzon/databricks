{"cells":[{"cell_type":"code","source":["#  Replace with your container and storage account:  \"wasbs://<container>@<storage account>.blob.core.windows.net/\"\npathPrefix = \"wasbs://marvel@vpldb.blob.core.windows.net/\"\n#  Fetch porgat.txt from storage account\nfile = sc.textFile(pathPrefix + \"porgat.txt\")"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#  Remove the headers from the file:  lines starting with a star\nnoHeaders = file.filter(lambda x: len(x)>0 and x[0]!='*')\n#  Extract a pair from each line:  the leading integer and a string for the rest of the line\npaired = noHeaders.map(lambda l:  l.partition(' ')).filter(lambda t:  len(t)==3 and len(t[0])>0 and len(t[2])>0).map(lambda t: (int(t[0]), t[2]))\n#  Filter relationships as they do not start with quotes, then split the integer list\nscatteredRelationships = paired.filter(lambda (charId, text):  text[0]!='\"').map(lambda (charId, text): (charId, [int(x) for x in text.split(' ')]))\n#  Relationships for the same character id sometime spans more than a line in the file, so let's group them together\nrelationships = scatteredRelationships.reduceByKey(lambda pubList1, pubList2: pubList1 + pubList2)\n#  Filter non-relationships as they start with quotes ; remove the quotes\nnonRelationships = paired.filter(lambda (index, text):  text[0]=='\"').map(lambda (index, text):  (index, text[1:-1].strip()))\n#  Characters stop at a certain line (part of the initial header ; we hardcode it here)\ncharacters = nonRelationships.filter(lambda (charId, name): charId<=6486)\n#  Publications starts after the characters\npublications = nonRelationships.filter(lambda (charId, name): charId>6486)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#  Let's find the characters appearing together most often\n\n#  Let's take the relationship RDD and do a cartesian product with itself all possible duos ; we repartition to be able to scale\nproduct = relationships.repartition(100).cartesian(relationships)\n#  Let's then remap it to have the character ids together and intersect their publications (using Python's sets)\nremapped = product.map(lambda ((charId1, pubList1), (charId2, pubList2)): ((charId1, charId2), list(set(pubList1) & set(pubList2))))\n#  Let's eliminate doublons\nnoDoublons = remapped.filter(lambda ((charId1, charId2), pubList): charId1<charId2)\n#  Let's remove empty publication list\nnoEmptyPublications = noDoublons.filter(lambda ((charId1, charId2), pubList): len(pubList)>0)\n#  Let's flip the mapping in order to sort by length of publications & drop the publication lists themselves\nsorted = noEmptyPublications.map(lambda ((charId1, charId2), pubList): (len(pubList), (charId1, charId2))).sortByKey(False)\n#  Action:  let's output the first 10 results\ntop10 = sorted.take(10)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#  Join once for the first character ; we first need to flip the RDD to have charId1 as the key\nname1 = sc.parallelize(top10).map(lambda (pubCount, (charId1, charId2)): (charId1, (charId2, pubCount))).join(characters)\n#  Let's perform a similar join on the second character\nname2 = name1.map(lambda (charId1, ((charId2, pubCount), name1)): (charId2, (name1, charId1, pubCount))).join(characters)\n#  Let's format the RDD a bit\nformattedTop10 = name2.map(lambda (charId2, ((name1, charId1, pubCount), name2)): (pubCount, (name1, charId1, name2, charId2)))\n\n#  We need to sort the results again:  when we parallelized the top10 it got partitionned and each partition moved independantly\nformattedTop10.sortByKey(False).collect()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":5}],"metadata":{"name":"RDD Blog","notebookId":3185064906819830},"nbformat":4,"nbformat_minor":0}
